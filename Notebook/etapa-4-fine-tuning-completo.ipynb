{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Importando as bibliotecas***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Your Flash Attention 2 installation seems to be broken?\n",
      "A possible explanation is you have a new CUDA version which isn't\n",
      "yet compatible with FA2? Please file a ticket to Unsloth or FA2.\n",
      "We shall now use Xformers instead, which does not have any performance hits!\n",
      "We found this negligible impact by benchmarking on 1x A100.\n",
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import shutil\n",
    "import gc\n",
    "from datasets import load_dataset\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Inicializando o Foundation Model***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Foundation Model escolhido para o treinamento ser√° o **unsloth/tinyllama-bnb-4bit** por conta do tamanho e a utiliza√ß√£o da mem√≥ria. O max_seq_length ser√° utilizado se baseando na quantidade de tokens que retornou na etapa 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 256 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Carregando os arquivos do dataset***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como aqui ser√° feito o treinamento completo, ou seja, com todas as partes do dataset, √© criado uma variavel com a lista de nome destes arquivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista com os nomes dos arquivos\n",
    "arquivos = [f'./LF-Amazon-1.3M/treino/trn_formatado_parte_{num}.json' for num in range(1, 11)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Fine Tuning***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Argumentos de treinamento**\n",
    "\n",
    "Os argumentos que ser√£o utilizados aqui s√£o os argumentos finais que foram testados na etapa 3. Com a seguinte diferen√ßa:\n",
    "- *num_train_epochs*: Ser√° utilizado 1 √©poca por conta do tempo mas, ao menos, ira garantir que o modelo viu todos os dados do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par√¢metros para o treinamento\n",
    "trainingArguments = TrainingArguments(\n",
    "    per_device_train_batch_size=128,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=5,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=3e-4,\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    logging_steps=100,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fun√ß√£o de treinamento**\n",
    "\n",
    "Foi realizado algumas modifica√ß√µes comparado com o da etapa 4, ficando da seguinte forma:\n",
    "\n",
    "**1.** Inicializa√ß√£o do modelo.\n",
    "- Valida a parte e inicia o modelo, caso for a primeira parte o treinamento ser√° feito no Foundation Model que seria o TinyLlama, ap√≥s a primeira parte o treinamento come√ßa a ser realizado no modelo salvo.\n",
    "- Foi feito desta forma para que, ao iniciar o treinamento de um novo dataset, limpe a mem√≥ria da GPU e inicie o ultimo modelo treinado.\n",
    "\n",
    "**2.** Carregamento do dataset.\n",
    "- Inicia o dataset que foi passado como par√¢metro.\n",
    "\n",
    "**3.** Cria√ß√£o do trainer.\n",
    "- Aqui neste caso n√£o vai ser realizado a avalia√ß√£o do modelo, apenas o treinamento.\n",
    "\n",
    "**4.** Realiza√ß√£o do treinamento.\n",
    "\n",
    "- Inicia o treinamento do modelo com o dataset que foi passado.\n",
    "\n",
    "**5.** Salvar o modelo.\n",
    "- Por √∫ltimo √© realizado a grava√ß√£o do modelo em uma pasta, fazendo uma identifica√ß√£o de quantas partes foram treinadas at√© o momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar_e_limpar_memoria(arquivo, parte):\n",
    "    print(f\"Lendo e treinando com o arquivo: {arquivo}\")\n",
    "    \n",
    "    if parte == 0:\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = \"unsloth/tinyllama-bnb-4bit\",\n",
    "            max_seq_length = max_seq_length,\n",
    "            dtype = dtype,\n",
    "            load_in_4bit = load_in_4bit,\n",
    "        )\n",
    "        \n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "            target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                            \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "            lora_alpha = 16,\n",
    "            lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "            bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "            # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "            use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "            random_state = 3407,\n",
    "            use_rslora = False,  # We support rank stabilized LoRA\n",
    "            loftq_config = None, # And LoftQ\n",
    "        )\n",
    "    else:\n",
    "        torch.cuda.empty_cache()  # Limpar a mem√≥ria da GPU\n",
    "        gc.collect()  # For√ßar a coleta de lixo\n",
    "        \n",
    "        # Carregar o √∫ltimo modelo treinado\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = f\"./model/lora_model_parte_{parte}\",\n",
    "            max_seq_length = max_seq_length,\n",
    "            dtype = dtype,\n",
    "            load_in_4bit = load_in_4bit,\n",
    "        )\n",
    "    \n",
    "    # Carregar o dataset a partir do arquivo JSON\n",
    "    dataset = load_dataset(\"json\", data_files=arquivo, split=\"train\")\n",
    "    \n",
    "    # Configurar o treinador para cada parte do dataset\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dataset_num_proc=8,  # Multiprocessamento\n",
    "        packing=False,  # Se necess√°rio para sequ√™ncias curtas\n",
    "        args=trainingArguments\n",
    "    )\n",
    "    \n",
    "    # Treinar o modelo\n",
    "    trainer.train()\n",
    "    \n",
    "    if parte == 9:\n",
    "        # Caminho da pasta do modelo\n",
    "        pasta_modelo = f\"./model/lora_model_final\"\n",
    "    else:\n",
    "        # Caminho da pasta do modelo\n",
    "        pasta_modelo = f\"./model/lora_model_parte_{parte + 1}\"\n",
    "    \n",
    "    # Salvar o melhor modelo treinado automaticamente\n",
    "    trainer.save_model(pasta_modelo)\n",
    "    tokenizer.save_pretrained(pasta_modelo)\n",
    "    \n",
    "    print(f\"Treinamento com {arquivo} conclu√≠do.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Treinamento**\n",
    "\n",
    "Para cada arquivo que foram recuperados na vari√°vel *arquivos* √© realizado um treinamento em cima do modelo. Tamb√©m √© passado qual parte que esta sendo treinada para a identifica√ß√£o do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo e treinando com o arquivo: ./LF-Amazon-1.3M/treino/trn_formatado_parte_1.json\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070 Ti. Max memory: 11.994 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 22 layers with 22 QKV layers, 22 O layers and 22 MLP layers.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 139,041 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 128 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 256 | Total steps = 543\n",
      " \"-____-\"     Number of trainable parameters = 12,615,680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='543' max='543' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [543/543 1:34:14, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.725300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.559700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.552100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.543800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento com ./LF-Amazon-1.3M/treino/trn_formatado_parte_1.json conclu√≠do.\n",
      "\n",
      "Lendo e treinando com o arquivo: ./LF-Amazon-1.3M/treino/trn_formatado_parte_2.json\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070 Ti. Max memory: 11.994 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 139,041 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 128 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 256 | Total steps = 543\n",
      " \"-____-\"     Number of trainable parameters = 12,615,680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='543' max='543' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [543/543 1:34:54, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.539400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.527700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.520900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.514000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.516900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento com ./LF-Amazon-1.3M/treino/trn_formatado_parte_2.json conclu√≠do.\n",
      "\n",
      "Lendo e treinando com o arquivo: ./LF-Amazon-1.3M/treino/trn_formatado_parte_3.json\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070 Ti. Max memory: 11.994 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 139,041 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 128 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 256 | Total steps = 543\n",
      " \"-____-\"     Number of trainable parameters = 12,615,680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='543' max='543' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [543/543 1:34:35, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.514500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.509800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.498000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.495800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.490300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento com ./LF-Amazon-1.3M/treino/trn_formatado_parte_3.json conclu√≠do.\n",
      "\n",
      "Lendo e treinando com o arquivo: ./LF-Amazon-1.3M/treino/trn_formatado_parte_4.json\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070 Ti. Max memory: 11.994 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 139,040 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 128 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 256 | Total steps = 543\n",
      " \"-____-\"     Number of trainable parameters = 12,615,680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='543' max='543' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [543/543 1:34:12, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.492300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.484200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.483800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.480200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.473800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento com ./LF-Amazon-1.3M/treino/trn_formatado_parte_4.json conclu√≠do.\n",
      "\n",
      "Lendo e treinando com o arquivo: ./LF-Amazon-1.3M/treino/trn_formatado_parte_5.json\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070 Ti. Max memory: 11.994 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 139,040 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 128 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 256 | Total steps = 543\n",
      " \"-____-\"     Number of trainable parameters = 12,615,680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='543' max='543' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [543/543 1:34:08, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.481300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.474900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.469100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.464400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.464900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento com ./LF-Amazon-1.3M/treino/trn_formatado_parte_5.json conclu√≠do.\n",
      "\n",
      "Lendo e treinando com o arquivo: ./LF-Amazon-1.3M/treino/trn_formatado_parte_6.json\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070 Ti. Max memory: 11.994 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 139,040 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 128 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 256 | Total steps = 543\n",
      " \"-____-\"     Number of trainable parameters = 12,615,680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='543' max='543' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [543/543 1:32:18, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.467400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.462200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.457100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.449800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.454200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento com ./LF-Amazon-1.3M/treino/trn_formatado_parte_6.json conclu√≠do.\n",
      "\n",
      "Lendo e treinando com o arquivo: ./LF-Amazon-1.3M/treino/trn_formatado_parte_7.json\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070 Ti. Max memory: 11.994 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 139,040 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 128 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 256 | Total steps = 543\n",
      " \"-____-\"     Number of trainable parameters = 12,615,680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='543' max='543' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [543/543 1:30:17, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.457700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.453900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.452800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.446700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento com ./LF-Amazon-1.3M/treino/trn_formatado_parte_7.json conclu√≠do.\n",
      "\n",
      "Lendo e treinando com o arquivo: ./LF-Amazon-1.3M/treino/trn_formatado_parte_8.json\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070 Ti. Max memory: 11.994 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 139,040 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 128 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 256 | Total steps = 543\n",
      " \"-____-\"     Number of trainable parameters = 12,615,680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='543' max='543' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [543/543 1:30:13, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.449900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.445700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.440500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.436700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.436600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento com ./LF-Amazon-1.3M/treino/trn_formatado_parte_8.json conclu√≠do.\n",
      "\n",
      "Lendo e treinando com o arquivo: ./LF-Amazon-1.3M/treino/trn_formatado_parte_9.json\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070 Ti. Max memory: 11.994 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 139,040 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 128 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 256 | Total steps = 543\n",
      " \"-____-\"     Number of trainable parameters = 12,615,680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='543' max='543' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [543/543 1:30:10, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.434300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.440700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.434800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento com ./LF-Amazon-1.3M/treino/trn_formatado_parte_9.json conclu√≠do.\n",
      "\n",
      "Lendo e treinando com o arquivo: ./LF-Amazon-1.3M/treino/trn_formatado_parte_10.json\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070 Ti. Max memory: 11.994 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 139,040 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 128 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 256 | Total steps = 543\n",
      " \"-____-\"     Number of trainable parameters = 12,615,680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='543' max='543' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [543/543 1:30:21, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.433400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.429900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.430800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.427600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.420600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento com ./LF-Amazon-1.3M/treino/trn_formatado_parte_10.json conclu√≠do.\n",
      "\n",
      "Treinamento conclu√≠do.\n"
     ]
    }
   ],
   "source": [
    "for parte, arquivo in enumerate(arquivos):\n",
    "    treinar_e_limpar_memoria(arquivo, parte)\n",
    "    \n",
    "    parte += 1\n",
    "    \n",
    "print(\"Treinamento conclu√≠do.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo ./model compactado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Compactar a pasta do modelo em um arquivo zip\n",
    "pasta_modelo = f\"./model\"\n",
    "shutil.make_archive(base_name=pasta_modelo, format='zip', root_dir=pasta_modelo)   \n",
    "print(f\"Modelo {pasta_modelo} compactado com sucesso.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
