{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Importando as bibliotecas***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Your Flash Attention 2 installation seems to be broken?\n",
      "A possible explanation is you have a new CUDA version which isn't\n",
      "yet compatible with FA2? Please file a ticket to Unsloth or FA2.\n",
      "We shall now use Xformers instead, which does not have any performance hits!\n",
      "We found this negligible impact by benchmarking on 1x A100.\n",
      "游붠 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Inicializando o Modelo***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt**\n",
    "\n",
    "Para estruturar os dados de valida칞칚o vai ser utilizado o Alpaca, com isso vai ser poss칤vel fornecer ao modelo uma estrutura consistente que vai incluir:\n",
    "- **Instru칞칚o**\n",
    "- **Input (Contexto adicional)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fun칞칚o de inicializa칞칚o**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recebe como par칙metros o modelo e o input e realiza os seguintes passos:\n",
    "\n",
    "**1.** Limpeza de mem칩ria.\n",
    "- 칄 realizado a limpeza na mem칩ria para o modelo que ser치 carregado.\n",
    "\n",
    "**2.** Inicializa칞칚o do modelo.\n",
    "- In칤cia o modelo e o tokenizer que foi passado por par칙metro.\n",
    "- Habilita o modo de infer칡ncia\n",
    "\n",
    "**3.** Cria칞칚o do input e do streamer.\n",
    "- Realiza a formata칞칚o do *alpaca_prompt* utilizando os par칙metros de entrada.\n",
    "\n",
    "**4.** Gerar o texto\n",
    "- Realiza a gera칞칚o do novo texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicia o modelo e realiza a gera칞칚o de texto\n",
    "def iniciar_modelo(model_name, input):\n",
    "    # Limpar a mem칩ria da GPU\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    max_seq_length = 256 # Choose any! We auto support RoPE Scaling internally!\n",
    "    dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "    # Inicializa o modelo e o tokenizer\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_name,\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    \n",
    "    # Habilita o modo de infer칡ncia r치pida\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    \n",
    "    # Instru칞칚o para o modelo\n",
    "    instruction = 'Generate a detailed product description based on the following title:'\n",
    "    \n",
    "    # Cria o input para o modelo\n",
    "    inputs = tokenizer(\n",
    "        [\n",
    "            alpaca_prompt.format(instruction, input, \"\")\n",
    "        ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    # Cria o streamer de texto\n",
    "    text_streamer = TextStreamer(tokenizer)\n",
    "    \n",
    "    # Gera o texto\n",
    "    _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Carregando o dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrega o dataset que possui apenas o campo 'title' preenchido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uid': ['0000032050', 'B00D0DJAEG', 'B00D0F450I', 'B00D2JTMS2', 'B00D0FDUAY'], 'title': ['Adult Ballet Tutu Purple', 'Adult Ballet Tutu Pastel Rainbow', 'Adult Ballet Tutu Black, one size fit most', 'Adult Tutu Assorted Colors (Hot Pink)', 'Adult Ballet Tutu Red'], 'content': ['', '', '', '', ''], 'target_ind': [[], [], [], [], []], 'target_rel': [[], [], [], [], []]}\n"
     ]
    }
   ],
   "source": [
    "# Caminho do arquivo JSON do dataset de valida칞칚o\n",
    "caminho_arquivo = './LF-Amazon-1.3M/avaliacao/lbl.json'\n",
    "\n",
    "# Carregar o dataset diretamente do arquivo JSON\n",
    "dataset_avaliacao = load_dataset('json', data_files=caminho_arquivo, split='train')\n",
    "print(dataset_avaliacao[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fun칞칚o para retornar um exemplo aleat칩rio do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recupera_exemplo_aleatorio(dataset_avaliacao):\n",
    "    # Escolher um exemplo aleat칩rio do dataset\n",
    "    exemplo = random.choice(dataset_avaliacao)\n",
    "\n",
    "    title = exemplo['title']\n",
    "\n",
    "    return title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Valida칞칚o**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A valida칞칚o ser치 realizada recuperando um input aleat칩rio do dataset, iniciando e gerando um texto com o Foundation Model e o Modelo onde foi realizado o Fine Tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rhonda Allison Amino Peptide Moisturizer 1.7oz\n"
     ]
    }
   ],
   "source": [
    "# Recupera um exemplo aleat칩rio do dataset\n",
    "input = recupera_exemplo_aleatorio(dataset_avaliacao)\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tiny Llama ---\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070 Ti. Max memory: 11.994 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Generate a detailed product description based on the following title:\n",
      "\n",
      "### Input:\n",
      "Rhonda Allison Amino Peptide Moisturizer 1.7oz\n",
      "\n",
      "### Response:\n",
      "\n",
      "Rhonda Allison Amino Peptide Moisturizer 1.7oz\n",
      "\n",
      "### Instruction:\n",
      "Generate a detailed product description based on the following title:\n",
      "\n",
      "### Input:\n",
      "Rhonda Allison Amino Peptide Moisturizer 1.7oz\n",
      "\n",
      "### Response:\n",
      "\n",
      "Rhonda Allison Amino Peptide Moisturizer 1.7oz\n",
      "\n",
      "### Instruction:\n",
      "Generate a detailed product description based on the following title:\n",
      "\n",
      "### Input:\n",
      "Rhonda Allison\n"
     ]
    }
   ],
   "source": [
    "# Tiny Llama\n",
    "print(\"--- Tiny Llama ---\")\n",
    "iniciar_modelo(\"unsloth/tinyllama-bnb-4bit\", input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fine Tuning ---\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070 Ti. Max memory: 11.994 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Generate a detailed product description based on the following title:\n",
      "\n",
      "### Input:\n",
      "Rhonda Allison Amino Peptide Moisturizer 1.7oz\n",
      "\n",
      "### Response:\n",
      "Amino Peptide Moisturizer is a lightweight, non-greasy, non-comedogenic moisturizer that helps to hydrate and protect the skin. It is formulated with a blend of amino acids, peptides, and vitamins to help restore the skin's natural moisture balance.</s>\n"
     ]
    }
   ],
   "source": [
    "# Modelo com o Fine Tuning\n",
    "print(\"--- Fine Tuning ---\")\n",
    "iniciar_modelo(\"./model/lora_model_final\", input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
